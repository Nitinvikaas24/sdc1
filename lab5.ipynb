{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a484a4df",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import gradio as gr\n",
    "\n",
    "# 3. Sample Data (Minimal dataset for demonstration)\n",
    "data = {\n",
    "    'text': [\n",
    "        \"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005.\", # Spam\n",
    "        \"WINNER!! As a valued network customer you have been selected to receivea Â£900 prize reward!\", # Spam\n",
    "        \"Had your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free!\", # Spam\n",
    "        \"Congrats! You have won a free vacation. Click here\", # Spam\n",
    "        \"URGENT! Your account needs immediate attention. Click link.\", # Spam\n",
    "        \"Nah I don't think he goes to usf, he lives around here though\", # Ham\n",
    "        \"Even my brother is not like to speak with me. They treat me like aids patent.\", # Ham\n",
    "        \"I'm going to try for 2 months ha ha only joking\", # Ham\n",
    "        \"Ok lar... Joking wif u oni...\", # Ham\n",
    "        \"Hi, how are you doing today?\", # Ham\n",
    "        \"Meeting scheduled for 3 PM tomorrow.\", # Ham\n",
    "        \"Can you please send me the report?\", # Ham\n",
    "    ],\n",
    "    'label': [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0] # 1 for Spam, 0 for Not Spam (Ham)\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 4. Create and Train the Model Pipeline\n",
    "# Uses CountVectorizer to convert text to word counts\n",
    "# Uses Multinomial Naive Bayes classifier\n",
    "model = make_pipeline(CountVectorizer(), MultinomialNB())\n",
    "model.fit(df['text'], df['label'])\n",
    "\n",
    "# 5. Define the prediction function for Gradio\n",
    "def predict_spam(text):\n",
    "    \"\"\"\n",
    "    Predicts if the input text is spam or not spam.\n",
    "    Args:\n",
    "        text (str): The input text message.\n",
    "    Returns:\n",
    "        str: Prediction result (\"Spam\" or \"Not Spam\").\n",
    "    \"\"\"\n",
    "    prediction = model.predict([text]) # Predict expects an iterable\n",
    "    if prediction[0] == 1:\n",
    "        return \"ðŸš¨ Spam Detected!\"\n",
    "    else:\n",
    "        return \"âœ… Not Spam (Ham).\"\n",
    "\n",
    "# 6. Create the Gradio interface\n",
    "iface_spam = gr.Interface(\n",
    "    fn=predict_spam,\n",
    "    inputs=gr.Textbox(lines=5, label=\"Enter Text Message\", placeholder=\"Type your message here...\"),\n",
    "    outputs=gr.Textbox(label=\"Prediction Result\"),\n",
    "    title=\"ðŸ“§ Spam Detection Classifier\",\n",
    "    description=\"Enter a text message to classify it as Spam or Not Spam using a Naive Bayes model.\",\n",
    "    examples=[\n",
    "        [\"Free prize! Click now!\"],\n",
    "        [\"Hi Mom, running late for dinner.\"],\n",
    "        [\"Your order has shipped.\"]\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 7. Launch the interface\n",
    "print(\"Launching Spam Detection Gradio Interface...\")\n",
    "iface_spam.launch(share=True) # share=True provides a public"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ddd899",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "# Removed 'Conversation' as it's not directly importable this way and wasn't used in the function below\n",
    "from transformers import pipeline\n",
    "import torch # Ensure torch is available\n",
    "\n",
    "# 3. Load a pre-trained conversational model pipeline\n",
    "# Using a smaller model suitable for Colab environments\n",
    "# You might need to accept terms or log in to Hugging Face for some models\n",
    "print(\"Loading text-generation model...\")\n",
    "try:\n",
    "    # Using DialoGPT-medium as an example.\n",
    "    # *** CHANGED task from \"conversational\" to \"text-generation\" ***\n",
    "    chatbot_pipeline = pipeline(\"text-generation\", model=\"microsoft/DialoGPT-medium\", pad_token_id=50256) # Added pad_token_id for open-ended generation\n",
    "    print(\"Model loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    print(\"Please ensure you have accepted model terms on Hugging Face if required, or try a different model.\")\n",
    "    # Fallback or exit if model loading fails\n",
    "    chatbot_pipeline = None # Set to None if loading failed\n",
    "\n",
    "# 4. Define the prediction function for Gradio ChatInterface\n",
    "# Note: The simple approach below relies on the pipeline's internal state\n",
    "# and basic text generation. It might not be truly conversational without\n",
    "# explicitly managing history input to the model.\n",
    "\n",
    "def chatbot_response(message, history):\n",
    "    \"\"\"\n",
    "    Generates a response from the chatbot model using text-generation pipeline.\n",
    "    Args:\n",
    "        message (str): The user's input message.\n",
    "        history (list): A list of previous user/bot message pairs (provided by Gradio, format depends on chatbot type).\n",
    "    Returns:\n",
    "        str: The chatbot's generated response.\n",
    "    \"\"\"\n",
    "    if chatbot_pipeline is None:\n",
    "        return \"Error: Chatbot model could not be loaded.\"\n",
    "\n",
    "    # Simple approach for text-generation pipeline:\n",
    "    # Pass the message, generate text, and try to extract the response part.\n",
    "    # This is basic and might include the prompt; better methods involve chat templates or direct model generation.\n",
    "    try:\n",
    "        # Generate text based on the input message\n",
    "        # max_length can be adjusted; setting it slightly longer than input helps get a response\n",
    "        responses = chatbot_pipeline(message, max_length=100, num_return_sequences=1)\n",
    "\n",
    "        if responses and len(responses) > 0 and 'generated_text' in responses[0]:\n",
    "             generated_text = responses[0]['generated_text']\n",
    "             # Try to extract only the newly generated part (heuristic)\n",
    "             # This assumes the response starts with the input message, which is common but not guaranteed.\n",
    "             if generated_text.startswith(message):\n",
    "                 # Remove the input prompt and potential leading/trailing spaces\n",
    "                 bot_message = generated_text[len(message):].strip()\n",
    "                 # Handle cases where only the prompt was returned or empty response\n",
    "                 if not bot_message:\n",
    "                      bot_message = \"...\" # Or some default response\n",
    "             else:\n",
    "                 # If the prompt isn't there, return the whole generation (might be okay)\n",
    "                 bot_message = generated_text.strip()\n",
    "\n",
    "             # Avoid returning empty strings\n",
    "             if not bot_message:\n",
    "                bot_message = \"I'm not sure how to respond to that.\"\n",
    "\n",
    "        else:\n",
    "            print(\"Pipeline did not return expected output format.\")\n",
    "            bot_message = \"Sorry, I couldn't generate a response.\"\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during chatbot inference: {e}\")\n",
    "        bot_message = \"Sorry, I encountered an error processing your message.\"\n",
    "\n",
    "    return bot_message\n",
    "\n",
    "# 5. Create and launch the Gradio Chat Interface\n",
    "print(\"Launching Chatbot Gradio Interface...\")\n",
    "iface_chat = gr.ChatInterface(\n",
    "    fn=chatbot_response,\n",
    "    title=\"ðŸ’¬ Simple LLM Chatbot (DialoGPT)\",\n",
    "    description=\"Chat with a conversational AI model (using text-generation). Type your message and press Enter.\",\n",
    "    examples=[[\"Hello!\"], [\"How does photosynthesis work?\"], [\"Tell me a joke.\"]],\n",
    "    # *** ADDED type='messages' to gr.Chatbot to address UserWarning ***\n",
    "    chatbot=gr.Chatbot(height=400, type='messages'), # Customize chatbot display height\n",
    "    textbox=gr.Textbox(placeholder=\"Ask me anything...\", container=False, scale=7),\n",
    "    # *** REMOVED retry_btn argument as it caused TypeError ***\n",
    "    # *** REMOVED undo_btn and clear_btn arguments to simplify the call ***\n",
    "    # undo_btn=\"Delete Previous\", # This argument seems valid but removed for testing\n",
    "    # clear_btn=\"Clear Chat\",    # This argument seems valid but removed for testing\n",
    ")\n",
    "\n",
    "iface_chat.launch(share=True) # share=True provides a public link\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e788675",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gradio as gr\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "# Import Google AI components\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import google.generativeai as genai # Import google-generativeai for configuration\n",
    "\n",
    "# --- Access Google AI API Key from Colab Secrets ---\n",
    "from google.colab import userdata\n",
    "\n",
    "try:\n",
    "    # *** LOOKING FOR GOOGLE_API_KEY now ***\n",
    "    google_api_key = userdata.get('GOOGLE_API_KEY')\n",
    "    # Configure the google-generativeai library\n",
    "    genai.configure(api_key=google_api_key)\n",
    "    print(\"Google AI API Key loaded and configured successfully.\")\n",
    "except userdata.SecretNotFoundError:\n",
    "    print(\"GOOGLE_API_KEY secret not found. Please add it in Colab Secrets.\")\n",
    "    print(\"Get a key from https://aistudio.google.com/app/apikey\")\n",
    "    google_api_key = None\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred accessing secrets or configuring Google AI: {e}\")\n",
    "    google_api_key = None\n",
    "# ----------------------------------------------------\n",
    "\n",
    "# 3. Prepare Document Data (Example about Chennai)\n",
    "# Create a dummy text file in Colab's environment\n",
    "chennai_text = \"\"\"\n",
    "Chennai, formerly known as Madras, is the capital city of the Indian state of Tamil Nadu.\n",
    "Located on the Coromandel Coast off the Bay of Bengal, it is one of the largest cultural, economic and educational centres of south India.\n",
    "Chennai is known as the \"Detroit of India\" for its automobile industry. It is also a major centre for music, art and culture.\n",
    "The city is home to historical landmarks like Fort St. George, built in 1644, and several ancient temples.\n",
    "Marina Beach in Chennai is one of the longest urban beaches in the world.\n",
    "The climate is typically hot and humid for most of the year.\n",
    "Key areas include Mylapore, T. Nagar, Adyar, and George Town.\n",
    "\"\"\"\n",
    "with open(\"chennai_info.txt\", \"w\") as f:\n",
    "    f.write(chennai_text)\n",
    "\n",
    "# 4. Setup Langchain Components (only if key is available)\n",
    "retrieval_chain = None # Initialize retrieval_chain to None\n",
    "if google_api_key:\n",
    "    print(\"Setting up Langchain components with Google AI...\")\n",
    "    try:\n",
    "        # Load Document\n",
    "        loader = TextLoader(\"./chennai_info.txt\")\n",
    "        docs = loader.load()\n",
    "\n",
    "        # Split Document\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "        split_docs = text_splitter.split_documents(docs)\n",
    "\n",
    "        # Create Embeddings (Using Google's embedding model)\n",
    "        print(\"Loading Google AI embedding model...\")\n",
    "        # *** USING GoogleGenerativeAIEmbeddings ***\n",
    "        embedding_model = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "        # Create Vector Store\n",
    "        print(\"Creating vector store...\")\n",
    "        vector_store = FAISS.from_documents(split_docs, embedding_model)\n",
    "\n",
    "        # Create Retriever\n",
    "        retriever = vector_store.as_retriever(search_kwargs={\"k\": 2}) # Retrieve top 2 relevant chunks\n",
    "\n",
    "        # Define LLM (Using Google's Gemini Pro model via Langchain)\n",
    "        print(\"Setting up Google AI LLM (Gemini)...\")\n",
    "        # *** USING ChatGoogleGenerativeAI ***\n",
    "        llm = ChatGoogleGenerativeAI(model=\"gemini-pro\",\n",
    "                                     temperature=0.7, # Adjust creativity\n",
    "                                     convert_system_message_to_human=True) # Helps with some prompt structures\n",
    "\n",
    "        # Define Prompt Template\n",
    "        template = \"\"\"\n",
    "        Answer the user's question based only on the following context:\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question: {input}\n",
    "        Answer:\n",
    "        \"\"\"\n",
    "        prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "        # Create Chains\n",
    "        document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "        retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "        print(\"Langchain setup complete.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during Langchain setup with Google AI: {e}\")\n",
    "        retrieval_chain = None # Ensure chain is None if setup fails\n",
    "\n",
    "else:\n",
    "    print(\"Skipping Langchain setup as GOOGLE_API_KEY is missing.\")\n",
    "    # retrieval_chain remains None\n",
    "\n",
    "# 5. Define the Q&A function for Gradio\n",
    "def answer_question(question):\n",
    "    \"\"\"\n",
    "    Answers a question based on the loaded document using Langchain.\n",
    "    Args:\n",
    "        question (str): The user's question.\n",
    "    Returns:\n",
    "        str: The answer generated by the Langchain chain or an error message.\n",
    "    \"\"\"\n",
    "    if retrieval_chain is None:\n",
    "        # Updated error message\n",
    "        return \"Error: Langchain components not initialized. Please ensure GOOGLE_API_KEY secret is set and valid.\"\n",
    "    if not question:\n",
    "        return \"Please ask a question.\"\n",
    "\n",
    "    try:\n",
    "        print(f\"Invoking chain with question: {question}\")\n",
    "        response = retrieval_chain.invoke({\"input\": question})\n",
    "        print(f\"Chain response: {response}\")\n",
    "        # The actual answer is usually in the 'answer' key of the response dictionary\n",
    "        return response.get(\"answer\", \"Sorry, I couldn't generate an answer based on the context.\")\n",
    "    except Exception as e:\n",
    "        # Catch potential API errors from Google AI as well\n",
    "        print(f\"Error during Langchain invocation: {e}\")\n",
    "        error_message = f\"An error occurred: {e}\"\n",
    "        # Provide more specific feedback if it's a known API issue (optional)\n",
    "        if \"API key not valid\" in str(e):\n",
    "             error_message = \"Error: The provided Google AI API Key is not valid. Please check it in Colab Secrets.\"\n",
    "        elif \"quota\" in str(e).lower():\n",
    "             error_message = \"Error: You may have exceeded your Google AI API quota.\"\n",
    "        return error_message\n",
    "\n",
    "# 6. Create the Gradio interface\n",
    "print(\"Launching Langchain Q&A Gradio Interface (using Google AI)...\")\n",
    "iface_langchain_google = gr.Interface(\n",
    "    fn=answer_question,\n",
    "    inputs=gr.Textbox(lines=2, label=\"Ask a Question about Chennai\", placeholder=\"e.g., What is Chennai known for?\"),\n",
    "    outputs=gr.Textbox(label=\"Answer\"),\n",
    "    title=\"ðŸ“š Langchain Q&A about Chennai (Google AI/Gemini)\",\n",
    "    description=\"Ask questions based on a small text about Chennai. Uses RAG with Google Gemini. Requires GOOGLE_API_KEY.\",\n",
    "    examples=[\n",
    "        [\"What is Marina Beach?\"],\n",
    "        [\"Which industries is Chennai known for?\"],\n",
    "        [\"Where is Fort St. George located?\"]\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 7. Launch the interface\n",
    "iface_langchain_google.launch(share=True)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
